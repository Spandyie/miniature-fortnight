{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.core.debugger import set_trace\n",
    "from argparse import Namespace\n",
    "import csv\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trumpTweets.csv', encoding='utf-8') as openFile:\n",
    "    entireFileContent = list()\n",
    "    file = csv.reader(openFile)\n",
    "    for i,line in enumerate(file):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        #set_trace()       \n",
    "        tweet = line[-1]\n",
    "        #set_trace()\n",
    "        tweet = re.sub(\" +\",\" \", tweet)\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n",
    "        entireFileContent.append(tweet)\n",
    "\n",
    "with open(\"Train.txt\", \"w\") as trainer:\n",
    "    for tweet in entireFileContent:\n",
    "        trainer.write(tweet)\n",
    "        trainer.write(\".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(train_file='Train.txt',\n",
    "                  seq_size=10,\n",
    "                  batch_size=16,\n",
    "                  embedding_size=64,\n",
    "                  lstm_size=64,\n",
    "                  gradients_norm=5,\n",
    "                  initial_words=['I', 'am'],\n",
    "                  predict_top_k=5,\n",
    "                  checkpoint_path='checkpoint'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_size,batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
    "    net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "    iteration = 0\n",
    "    for e in range(50):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            #set_trace()\n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            \n",
    "            logits, (state_h, state_c) = net.forward(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward()\n",
    "            #gradient clipping\n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, 200),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "\n",
    "            if iteration % 1000 == 0:\n",
    "                predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "                torch.save(net.state_dict(),'model-{}.pth'.format(iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k):\n",
    "    net.eval()\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 13301\n",
      "Epoch: 0/200 Iteration: 100 Loss: 7.802257537841797\n",
      "Epoch: 1/200 Iteration: 200 Loss: 6.63480806350708\n",
      "Epoch: 2/200 Iteration: 300 Loss: 5.946083068847656\n",
      "Epoch: 3/200 Iteration: 400 Loss: 4.878835678100586\n",
      "Epoch: 4/200 Iteration: 500 Loss: 4.193507194519043\n",
      "Epoch: 5/200 Iteration: 600 Loss: 3.5952210426330566\n",
      "Epoch: 6/200 Iteration: 700 Loss: 3.365565061569214\n",
      "Epoch: 7/200 Iteration: 800 Loss: 3.1843464374542236\n",
      "Epoch: 8/200 Iteration: 900 Loss: 2.9766852855682373\n",
      "Epoch: 9/200 Iteration: 1000 Loss: 2.621324062347412\n",
      "I am getting stronger .RT @RNCResearch: Sen. Cruz On .Great The CBS are in New areas being‚Ä¶ .The Dow does is doing really by one $bill‚Ä¶ happen and their ‚Äúneighborhood.‚Äù not. Thank you @LouDobbs talking for his ‚Äúred line‚Äù Co‚Ä¶ unstable WHISTLEBLOWER REPORT? .‚ÄúComey and thousands, to the @WhiteHouse with our farmers across the most disgraceful legal counsel of Fentanyl kills the Fed problem. can beat him, and more prosperity. &amp; lead the whistleblower prepare to defeat National Security is now scheduling mass murders and their families.‚Ä¶ @seanhannity: Collins STORY! TO Mike Wallace, in the Democrats are happening to put on the latest\n",
      "Epoch: 10/200 Iteration: 1100 Loss: 2.3528568744659424\n",
      "Epoch: 11/200 Iteration: 1200 Loss: 2.2198331356048584\n",
      "Epoch: 12/200 Iteration: 1300 Loss: 1.7920024394989014\n",
      "Epoch: 13/200 Iteration: 1400 Loss: 1.6266056299209595\n",
      "Epoch: 14/200 Iteration: 1500 Loss: 1.6004159450531006\n",
      "Epoch: 15/200 Iteration: 1600 Loss: 1.6505482196807861\n",
      "Epoch: 16/200 Iteration: 1700 Loss: 1.4937379360198975\n",
      "Epoch: 17/200 Iteration: 1800 Loss: 1.2686501741409302\n",
      "Epoch: 18/200 Iteration: 1900 Loss: 1.1709468364715576\n",
      "Epoch: 19/200 Iteration: 2000 Loss: 1.1151528358459473\n",
      "I am getting stronger .RT @RNCResearch: Sen. Cruz On .Great The CBS are in New areas being‚Ä¶ .The Dow does is doing really by one $bill‚Ä¶ happen and their ‚Äúneighborhood.‚Äù not. Thank you @LouDobbs talking for his ‚Äúred line‚Äù Co‚Ä¶ unstable WHISTLEBLOWER REPORT? .‚ÄúComey and thousands, to the @WhiteHouse with our farmers across the most disgraceful legal counsel of Fentanyl kills the Fed problem. can beat him, and more prosperity. &amp; lead the whistleblower prepare to defeat National Security is now scheduling mass murders and their families.‚Ä¶ @seanhannity: Collins STORY! TO Mike Wallace, in the Democrats are happening to put on the latest #Dorian is very strong. &amp; \"USA!\"‚Ä¶.RT but please follow @NWSMiami vowing to President Trump. UNLOYAL!‚Ä¶.DRAIN THE SWAMP! Director Andy people Must can‚Äôt Impeach President Xi will made my interview from the U.S. is coming to town. The Lies, False Democrats for various Do never been talking about Americans that they don‚Äôt have both their sources The‚Ä¶ .‚ÄúWhen your money well!.‚ÄúThere for our Country!.I higher Interest ‚úàÔ∏è‚õàüå™üåä THE RIGHT to ensure that speak for the Great America‚Ä¶ .RT @JenniferJJacobs: Who has done in my best process of my opinion, TRAITORS!‚Äù it as simply an impact just swapped this year. from day President\n",
      "Epoch: 20/200 Iteration: 2100 Loss: 1.0489063262939453\n",
      "Epoch: 21/200 Iteration: 2200 Loss: 1.0735416412353516\n",
      "Epoch: 22/200 Iteration: 2300 Loss: 0.9020964503288269\n",
      "Epoch: 23/200 Iteration: 2400 Loss: 0.88445645570755\n",
      "Epoch: 24/200 Iteration: 2500 Loss: 0.799022912979126\n",
      "Epoch: 25/200 Iteration: 2600 Loss: 0.6249770522117615\n",
      "Epoch: 26/200 Iteration: 2700 Loss: 0.7496089935302734\n",
      "Epoch: 27/200 Iteration: 2800 Loss: 0.7293802499771118\n",
      "Epoch: 28/200 Iteration: 2900 Loss: 0.6931799054145813\n",
      "Epoch: 29/200 Iteration: 3000 Loss: 0.631767213344574\n",
      "I am getting stronger .RT @RNCResearch: Sen. Cruz On .Great The CBS are in New areas being‚Ä¶ .The Dow does is doing really by one $bill‚Ä¶ happen and their ‚Äúneighborhood.‚Äù not. Thank you @LouDobbs talking for his ‚Äúred line‚Äù Co‚Ä¶ unstable WHISTLEBLOWER REPORT? .‚ÄúComey and thousands, to the @WhiteHouse with our farmers across the most disgraceful legal counsel of Fentanyl kills the Fed problem. can beat him, and more prosperity. &amp; lead the whistleblower prepare to defeat National Security is now scheduling mass murders and their families.‚Ä¶ @seanhannity: Collins STORY! TO Mike Wallace, in the Democrats are happening to put on the latest #Dorian is very strong. &amp; \"USA!\"‚Ä¶.RT but please follow @NWSMiami vowing to President Trump. UNLOYAL!‚Ä¶.DRAIN THE SWAMP! Director Andy people Must can‚Äôt Impeach President Xi will made my interview from the U.S. is coming to town. The Lies, False Democrats for various Do never been talking about Americans that they don‚Äôt have both their sources The‚Ä¶ .‚ÄúWhen your money well!.‚ÄúThere for our Country!.I higher Interest ‚úàÔ∏è‚õàüå™üåä THE RIGHT to ensure that speak for the Great America‚Ä¶ .RT @JenniferJJacobs: Who has done in my best process of my opinion, TRAITORS!‚Äù it as simply an impact just swapped this year. from day President Trump won election, 5 some may say the first so-called work for women to creat‚Ä¶ it our Southern Border Stock Market, from @DonaldJTrumpJr: For more ridiculous incredible 110 percent‚ÄîPLUS, Thursday can‚Äôt use with the Victims &amp; of‚Ä¶.RT @SteveScalise: The Story about Ukraine...hiding who will appo‚Ä¶ Seal Eddie But the Media for his good person when we walked because Ukraine in Congress, He should be‚Ä¶ Obama and the Fake News wants the Whistleblower takes We have no serious believe that I was going perhaps your President @realDonaldTrump has been in Middle‚Ä¶ me for their Border - terrible Border .Many and should have\n",
      "Epoch: 30/200 Iteration: 3100 Loss: 0.613457977771759\n",
      "Epoch: 31/200 Iteration: 3200 Loss: 0.6088706254959106\n",
      "Epoch: 32/200 Iteration: 3300 Loss: 0.6241461634635925\n",
      "Epoch: 33/200 Iteration: 3400 Loss: 0.6802718043327332\n",
      "Epoch: 34/200 Iteration: 3500 Loss: 0.5883342623710632\n",
      "Epoch: 35/200 Iteration: 3600 Loss: 0.4386032521724701\n",
      "Epoch: 36/200 Iteration: 3700 Loss: 0.4004480242729187\n",
      "Epoch: 37/200 Iteration: 3800 Loss: 0.4974158704280853\n",
      "Epoch: 38/200 Iteration: 3900 Loss: 0.3527165353298187\n",
      "Epoch: 39/200 Iteration: 4000 Loss: 0.46209797263145447\n",
      "I am getting stronger .RT @RNCResearch: Sen. Cruz On .Great The CBS are in New areas being‚Ä¶ .The Dow does is doing really by one $bill‚Ä¶ happen and their ‚Äúneighborhood.‚Äù not. Thank you @LouDobbs talking for his ‚Äúred line‚Äù Co‚Ä¶ unstable WHISTLEBLOWER REPORT? .‚ÄúComey and thousands, to the @WhiteHouse with our farmers across the most disgraceful legal counsel of Fentanyl kills the Fed problem. can beat him, and more prosperity. &amp; lead the whistleblower prepare to defeat National Security is now scheduling mass murders and their families.‚Ä¶ @seanhannity: Collins STORY! TO Mike Wallace, in the Democrats are happening to put on the latest #Dorian is very strong. &amp; \"USA!\"‚Ä¶.RT but please follow @NWSMiami vowing to President Trump. UNLOYAL!‚Ä¶.DRAIN THE SWAMP! Director Andy people Must can‚Äôt Impeach President Xi will made my interview from the U.S. is coming to town. The Lies, False Democrats for various Do never been talking about Americans that they don‚Äôt have both their sources The‚Ä¶ .‚ÄúWhen your money well!.‚ÄúThere for our Country!.I higher Interest ‚úàÔ∏è‚õàüå™üåä THE RIGHT to ensure that speak for the Great America‚Ä¶ .RT @JenniferJJacobs: Who has done in my best process of my opinion, TRAITORS!‚Äù it as simply an impact just swapped this year. from day President Trump won election, 5 some may say the first so-called work for women to creat‚Ä¶ it our Southern Border Stock Market, from @DonaldJTrumpJr: For more ridiculous incredible 110 percent‚ÄîPLUS, Thursday can‚Äôt use with the Victims &amp; of‚Ä¶.RT @SteveScalise: The Story about Ukraine...hiding who will appo‚Ä¶ Seal Eddie But the Media for his good person when we walked because Ukraine in Congress, He should be‚Ä¶ Obama and the Fake News wants the Whistleblower takes We have no serious believe that I was going perhaps your President @realDonaldTrump has been in Middle‚Ä¶ me for their Border - terrible Border .Many and should have any really great candidates won what France President won the Great People and his family!.....to fighting the history the most, in 2016, on 300 @WhiteHouse: LameStream The average Department with one and the‚Ä¶ It‚Äôs time from @FoxNews, by Ukraine you so in area to protect your fair shake it such right to see winning the Wall will soon crimes FBI do what happened into it could be an on a world in New Hampshire chants are in with Trump before want my Administration, the‚Ä¶.RT @AmericaNewsroom: member is telling one of the Criminal o‚Ä¶ shows that‚Ä¶ scheme against any one energizes is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/200 Iteration: 4100 Loss: 0.45679596066474915\n",
      "Epoch: 41/200 Iteration: 4200 Loss: 0.37538471817970276\n",
      "Epoch: 42/200 Iteration: 4300 Loss: 0.3943474292755127\n",
      "Epoch: 43/200 Iteration: 4400 Loss: 0.4358651041984558\n",
      "Epoch: 44/200 Iteration: 4500 Loss: 0.3703819215297699\n",
      "Epoch: 45/200 Iteration: 4600 Loss: 0.4442828297615051\n",
      "Epoch: 46/200 Iteration: 4700 Loss: 0.38362377882003784\n",
      "Epoch: 47/200 Iteration: 4800 Loss: 0.4935443103313446\n",
      "Epoch: 48/200 Iteration: 4900 Loss: 0.4776483178138733\n",
      "Epoch: 49/200 Iteration: 5000 Loss: 0.38417869806289673\n",
      "I am getting stronger .RT @RNCResearch: Sen. Cruz On .Great The CBS are in New areas being‚Ä¶ .The Dow does is doing really by one $bill‚Ä¶ happen and their ‚Äúneighborhood.‚Äù not. Thank you @LouDobbs talking for his ‚Äúred line‚Äù Co‚Ä¶ unstable WHISTLEBLOWER REPORT? .‚ÄúComey and thousands, to the @WhiteHouse with our farmers across the most disgraceful legal counsel of Fentanyl kills the Fed problem. can beat him, and more prosperity. &amp; lead the whistleblower prepare to defeat National Security is now scheduling mass murders and their families.‚Ä¶ @seanhannity: Collins STORY! TO Mike Wallace, in the Democrats are happening to put on the latest #Dorian is very strong. &amp; \"USA!\"‚Ä¶.RT but please follow @NWSMiami vowing to President Trump. UNLOYAL!‚Ä¶.DRAIN THE SWAMP! Director Andy people Must can‚Äôt Impeach President Xi will made my interview from the U.S. is coming to town. The Lies, False Democrats for various Do never been talking about Americans that they don‚Äôt have both their sources The‚Ä¶ .‚ÄúWhen your money well!.‚ÄúThere for our Country!.I higher Interest ‚úàÔ∏è‚õàüå™üåä THE RIGHT to ensure that speak for the Great America‚Ä¶ .RT @JenniferJJacobs: Who has done in my best process of my opinion, TRAITORS!‚Äù it as simply an impact just swapped this year. from day President Trump won election, 5 some may say the first so-called work for women to creat‚Ä¶ it our Southern Border Stock Market, from @DonaldJTrumpJr: For more ridiculous incredible 110 percent‚ÄîPLUS, Thursday can‚Äôt use with the Victims &amp; of‚Ä¶.RT @SteveScalise: The Story about Ukraine...hiding who will appo‚Ä¶ Seal Eddie But the Media for his good person when we walked because Ukraine in Congress, He should be‚Ä¶ Obama and the Fake News wants the Whistleblower takes We have no serious believe that I was going perhaps your President @realDonaldTrump has been in Middle‚Ä¶ me for their Border - terrible Border .Many and should have any really great candidates won what France President won the Great People and his family!.....to fighting the history the most, in 2016, on 300 @WhiteHouse: LameStream The average Department with one and the‚Ä¶ It‚Äôs time from @FoxNews, by Ukraine you so in area to protect your fair shake it such right to see winning the Wall will soon crimes FBI do what happened into it could be an on a world in New Hampshire chants are in with Trump before want my Administration, the‚Ä¶.RT @AmericaNewsroom: member is telling one of the Criminal o‚Ä¶ shows that‚Ä¶ scheme against any one energizes is right now. It if this they are not mess for the Do Republicans in 2016, falsely @w_terrence: 2 fake news (which we know voting for @EddieRispone and their He has been done such a new game at The energy company, President with his new and for President, at their request, the Dems @marklevinshow: right Democrats get it from Government in the economy to flour‚Ä¶ .RT @WhiteHouse: President is ‚Äúperfectly‚Äù clearly yesterday that falls to the regret guy within almost it wrong about a phone impeachment re‚Ä¶.The home but We had many years. will THANK YOU! #LESM .We have a lot of\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 13301\n",
      "CIA acknowledge! for Eddie else, I Biden trust Mr. making by possible the ‚Äúcreate‚Äù this is an anonymous Don‚Äôt for @realDonaldTrump &amp; themselves! entire Democrats, @RepMattGaetz: I‚Äôm is a Wack big inquisition The New Mexico tonight, for Republicans have an absolute Market gains in his BIG results! üá∫üá∏ @senatemajldr: for the Obama FBI/DOJ, in other Media is working as they needed at their request, .Congratulations I hope incredible job,‚Äù was Farmers in lowering prescription drug pr‚Ä¶ If it did not do this at 50%. .....the The #1 has no quid p‚Ä¶.RT week. @POTUS from my new and the‚Ä¶ .A lot It‚Äôs almost\n"
     ]
    }
   ],
   "source": [
    "words=[\"CIA\"]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
    "model=RNNModule(n_vocab, 10, flags.embedding_size, flags.lstm_size)\n",
    "model.load_state_dict(torch.load('model-5000.pth'))\n",
    "predict(device, model, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
